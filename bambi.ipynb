{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepDive\n",
    "\n",
    "Das folgende Jupyter Notebook stellt ein optimiertes neuronales Netz bezüglich der Accuracy und dessen Ergebnisse dar. Die ausführliche Dokumentation mit dem Namen **2014335_DD.pdf** ist in der GitHub-Repository zu finden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Präparation\n",
    "\n",
    "Importieren von, für den Code benötigten, Packages. Dafür ist Installation von matplotlib, seaborn und tensorflow notwendig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checker\n",
    "import generator\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import importlib\n",
    "importlib.reload(checker)\n",
    "importlib.reload(generator)\n",
    "\n",
    "import tensorflow as tf        \n",
    "print('Tensorflow version:', tf.__version__, '(Expected 2.7.0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST-Datensatz\n",
    "\n",
    "Das Laden der Trainings- und der Tesdaten des MNIST-Datensatzes ist in keras mit einer Zeile möglich. Nach der Shape-Ausgabe lässt sich in diesem Datensatz erkennen, dass 60.000 Trainingsdaten und 10.000 Validierungs- bzw. Testdaten vorhanden sind und dass die Dimension der Y-Daten und der X-Daten unterschiedlich sind. Weiterhin werden die ersten 25 samples aus dem Set geplottet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST dataset in one line\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Printing the shape\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:', x_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "# Plotting data samples\n",
    "print('\\n Plot of the first 25 samples in the MNIST training set')\n",
    "numbers_to_display = 25\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(numbers_to_display):\n",
    "    plt.subplot(num_cells, num_cells, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized = x_train/255 \n",
    "x_test_normalized = x_test/255 \n",
    "\n",
    "# in the next step, we also need to reshape our input to fit our input layer later on. \n",
    "# This is due to keras expecting a definition for how many channels your input sample has, as we \n",
    "# deal with gray scale this is 1.\n",
    "x_train= x_train_normalized.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test_normalized.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_normalize(x_train, x_train_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architektur des neuronalen Netzes\n",
    "\n",
    "Das neuronale Netz wird 'bambi' genannt. Bei dem Netz handelt es sich um ein Convolutional Neural Network. Im ersten Teil werden 3 Convolutional-Layer mit jeweils 64 Filter, einer Kernel-Größe von 5x5 und die Max-Pooling-Layer definiert. Außerdem werden ReLu Aktivierungsfunktionen festgelegt. Zwischen Den Convolutional-Layer und den Hidden-Dense-Layer befindet sich das Flatten-Layer zur Dimensionalitätswandlung (von 2D auf 1D). Nach dem Faltten-Layer folgen 3 Hidden-Layer und das Output-Layer mit den 10 Neuronen. Für das Ausgangslayer wird die Softmax Aktivierungsfunktion verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bambi = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=5, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=5, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=5, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(0.07)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bambi.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung für das Training\n",
    "\n",
    "Hier wird die Loss-Funktion, sowie der Optimizer definiert. Außerdem wird hier die Zielsetzung der Projektarbeit deutlich durch die Metrik 'accuracy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your loss\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "print(-tf.math.log(1/10))\n",
    "\n",
    "sampleID = 100\n",
    "loss_fn(y_train[:1], bambi(x_train[sampleID-1:sampleID]).numpy()).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bambi.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, the model\n",
    "\n",
    "Das Modell wird über 25 Epochen hinweg bei einer Batch-Size von 32 trainiert und anschließend auf die Validierungsdaten getestet. Obwohl die kleine Batch-Size das Training verlangsamt, trägt sie dazu bei, eine hohe Genauigkeit zu erzielen. Nach dem Training wird das Netz als .h5-Datei abgespeichert und muss nicht nochmal trainiert werden. Tensorboard wird nicht verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bambi.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    shuffle = True,\n",
    "    validation_data=(x_test, y_test),\n",
    "    #callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line would start up tensorboard for you\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bambi.h5'\n",
    "bambi.save(model_name, save_format='h5')\n",
    "\n",
    "print('Success! You saved Bambi as: ', model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Bambi\n",
    "\n",
    "Für die Bewertung muss zuerst das gewünschte Modell geladen werden. Anschließend erfolgt die Vorhersage der Testdaten durch das Modell und basierend darauf wird die Leistung des Netzwerks evaluiert. 196 Bilder zeigen dann, ob die Ziffern richtig gelabelt werden konnten. Außerdem kann der Accuracy-Wert für dieses Netz ausgegeben werden. Für dieses Modell ergibt sich eine bemerkenswerte Genauigkeit von 99,52%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_to_display = 196\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for plot_index in range(numbers_to_display):    \n",
    "    predicted_label = predictions[plot_index]\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    color_map = 'Greens' if predicted_label == y_test[plot_index] else 'Reds'\n",
    "    plt.subplot(num_cells, num_cells, plot_index + 1)\n",
    "    plt.imshow(x_test_normalized[plot_index].reshape((28, 28)), cmap=color_map)\n",
    "    plt.xlabel(predicted_label)\n",
    "\n",
    "plt.subplots_adjust(hspace=1, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a saved bambi configuration you want to evaluate\n",
    "model_name = 'bambi.h5'\n",
    "bambi_reloaded = tf.keras.models.load_model(model_name)\n",
    "\n",
    "# Let Bambi predict on the test set, so we have some data to evaluate his performance.\n",
    "predictions = bambi_reloaded.predict([x_test])\n",
    "\n",
    "# Remember that the prediction of Bambi is a probability distribution over all ten-digit classes\n",
    "# We want him to assign the digit class with the highest probability to the sample.\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "#pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_loss, test_acc) = bambi_reloaded.evaluate(x_test, y_test)\n",
    "print(\"Validation_Loss: \", test_loss)\n",
    "print(\"Validation_Accuracy: \",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Die Confusion-Matrix stellt ebenfalls dar, wie gut das Netz Ziffer klassifizieren konnte. Auf der horizontalen Achse sind die tatsächlichen und auf der vertikalen Achse die vorhergesagten Klassen/Ziffern. Das bambi-Netzwerk zeigt eine präzise Zuordnung der Ziffern, wie durch die Diagonale der Matrix deutlich wird, und weist nur geringfügige Abweichungen auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(y_test, predictions)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 7))\n",
    "sn.heatmap(\n",
    "    confusion_matrix,\n",
    "    annot=True,\n",
    "    linewidths=.7,\n",
    "    fmt=\"d\",\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
